{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Vim1NqJwWB",
        "outputId": "64924dbb-a906-4a74-d19f-cb70bb22f6f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------\n",
            "loading dataset...\n",
            "-----------------------------------------------------\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "num_samples, num_features (9982, 19)\n",
            "labels (9982,)\n",
            "-----------------------------------------------------\n",
            "Splitting dataset into train/test/val and normalizing\n",
            "-----------------------------------------------------\n",
            "Training features mean: -5.310618234022367e-16, std: 0.9999999999999959, shape: (6987, 19)\n",
            "Validation features mean: -0.0025389798364047877, std: 0.9022766022749542, shape: (1497, 19)\n",
            "Testing features mean: -0.012837901941694582, std: 1.055637237674046, shape: (1498, 19)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import sklearn.model_selection\n",
        "# makes printing more human-friendly\n",
        "np.set_printoptions(precision=3,suppress=True)\n",
        "\n",
        "#Function to process the dataset. Input the google drive path to the dataset as well as the percentage of train and test\n",
        "#Function outputs the standardized train, test, val datasets and labels as well as the std and mean used to standardize\n",
        "#Output is: X_train, y_train, X_val, y_val, X_test, y_test, mean, std\n",
        "def process_dataset(path='SolPred/curated-solubility-dataset.csv', train_percentage=0.7, test_percentage=0.15, val_percentage=0.15):\n",
        "\n",
        "\n",
        "  #Loads the database and takes out specific indices as labels and features. If changing dataset change those indexes\n",
        "  def loaddataset(path):\n",
        "      drive.mount('/content/drive/')\n",
        "      with open('/content/drive/MyDrive/'+path, 'r') as f:\n",
        "        l = csv.reader(f)\n",
        "        data = np.array([list(filter(None,i)) for i in l])\n",
        "        X = data[1:,9:-1]\n",
        "        y = data[1:,5]\n",
        "        return X.astype(np.double),y.astype(np.double)\n",
        "\n",
        "  #Splits the data into training data, validation data, and testing data, and normalizes the data with respect to the training portion\n",
        "  #Returns the X_train, y_train, X_val, y_val, X_test, y_val, train_mean, train_std\n",
        "  def traintestsplit_and_normalize(train_p, test_p, val_p, features, labels):\n",
        "    #Split data\n",
        "    X_train, X_temp, y_train, y_temp = sklearn.model_selection.train_test_split(features,labels, train_size = train_p, shuffle=True )\n",
        "    X_val, X_test, y_val, y_test = sklearn.model_selection.train_test_split(X_temp, y_temp, train_size = val_p/(val_p+test_p))\n",
        "\n",
        "    X_train = np.asfarray(X_train)\n",
        "    X_val = np.asfarray(X_val)\n",
        "    X_test = np.asfarray(X_test)\n",
        "\n",
        "    X_mean = np.mean(X_train, axis=0)\n",
        "    X_std = np.std(X_train, axis=0)\n",
        "\n",
        "    X_train = (X_train-X_mean)/X_std\n",
        "    X_val = (X_val-X_mean)/X_std\n",
        "    X_test = (X_test-X_mean)/X_std\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_mean, X_std\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  print('loading dataset...')\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  X,y = loaddataset(path)\n",
        "\n",
        "  #print(X[1])\n",
        "  print('num_samples, num_features', X.shape)\n",
        "  print('labels', y.shape)\n",
        "\n",
        "  print('-----------------------------------------------------')\n",
        "  print('Splitting dataset into train/test/val and normalizing')\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test, X_mean, X_std = traintestsplit_and_normalize(train_percentage,val_percentage,test_percentage,X,y)\n",
        "  print(\"Training features mean: \" + str(np.mean(X_train)) + \", std: \" + str(np.std(X_train)) + \", shape: \" + str(X_train.shape))\n",
        "  print(\"Validation features mean: \" + str(np.mean(X_val)) + \", std: \" + str(np.std(X_val)) + \", shape: \" + str(X_val.shape))\n",
        "  print(\"Testing features mean: \" + str(np.mean(X_test)) + \", std: \" + str(np.std(X_test)) + \", shape: \" + str(X_test.shape))\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test, X_mean, X_std\n",
        "\n",
        "#Processes dataset with default values\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, mean, std = process_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0p_UGOsSP2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}